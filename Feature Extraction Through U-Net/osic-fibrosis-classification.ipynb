{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":662983,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":501638,"modelId":516804}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI-Driven Early Prediction of Pulmonary Fibrosis Using Deep Learning\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 1: Feature Extraction Script","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pydicom\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"image_size\": 256,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"root_dir\": \"../input/osic-pulmonary-fibrosis-progression/train\",\n    # REPLACE THIS PATH with the actual path of your uploaded model in Kaggle\n    # /kaggle/input/u-net-classification/pytorch/default/1/epoch_16_f1_0.9021.pth\n    \"model_weights\": \"/kaggle/input/u-net-classification/pytorch/default/1/epoch_16_f1_0.9021.pth\", \n    \"batch_size\": 16\n}\n\n# ==========================================\n# 2. MODEL DEFINITION (Must match training)\n# ==========================================\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x): return self.double_conv(x)\n\nclass StandardUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        self.inc = DoubleConv(in_channels, 64)\n        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.conv_up1 = DoubleConv(1024, 512)\n        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.conv_up2 = DoubleConv(512, 256)\n        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.conv_up3 = DoubleConv(256, 128)\n        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.conv_up4 = DoubleConv(128, 64)\n        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5); x = torch.cat([x, x4], dim=1); x = self.conv_up1(x)\n        x = self.up2(x); x = torch.cat([x, x3], dim=1); x = self.conv_up2(x)\n        x = self.up3(x); x = torch.cat([x, x2], dim=1); x = self.conv_up3(x)\n        x = self.up4(x); x = torch.cat([x, x1], dim=1); x = self.conv_up4(x)\n        return torch.sigmoid(self.outc(x))\n\n# ==========================================\n# 3. EXTRACTION LOGIC\n# ==========================================\ndef process_patient(patient_id, model):\n    path = os.path.join(CONFIG['root_dir'], patient_id)\n    files = sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith('.dcm')])\n    \n    patient_metrics = {\n        'Patient': patient_id,\n        'lung_volume_ml': 0.0,\n        'fibrosis_mean_hu': -1000.0,\n        'fibrosis_std_hu': 0.0,\n        'slice_count': len(files)\n    }\n    \n    if not files: return patient_metrics\n\n    # Batch processing for speed\n    hu_values_accumulated = []\n    \n    for i in range(0, len(files), CONFIG['batch_size']):\n        batch_files = files[i : i + CONFIG['batch_size']]\n        batch_imgs = []\n        batch_metadata = []\n        \n        # Load and Preprocess\n        for f in batch_files:\n            try:\n                dcm = pydicom.dcmread(f)\n                img = dcm.pixel_array.astype(np.float32)\n                slope = getattr(dcm, 'RescaleSlope', 1)\n                intercept = getattr(dcm, 'RescaleIntercept', -1024)\n                img = slope * img + intercept\n                \n                # Metadata for Volume Calc\n                slice_thick = float(getattr(dcm, 'SliceThickness', 1.0))\n                # Pixel spacing is usually [row, col]\n                pixel_spacing = getattr(dcm, 'PixelSpacing', [1.0, 1.0])\n                spacing_x, spacing_y = float(pixel_spacing[0]), float(pixel_spacing[1])\n                \n                batch_metadata.append((slice_thick, spacing_x, spacing_y))\n                \n                # Resize for U-Net\n                img_resized = cv2.resize(img, (CONFIG['image_size'], CONFIG['image_size']))\n                \n                # Normalize (Same as Training)\n                img_norm = np.clip(img_resized, -1000, 400)\n                img_norm = (img_norm - (-1000)) / (400 - (-1000))\n                batch_imgs.append(img_norm)\n                \n            except:\n                continue\n        \n        if not batch_imgs: continue\n        \n        # Prediction\n        inp = torch.tensor(np.array(batch_imgs)).unsqueeze(1).float().to(CONFIG['device'])\n        with torch.no_grad():\n            preds = model(inp)\n            preds = (preds > 0.5).float().cpu().numpy().squeeze(1) # (B, 256, 256)\n            \n        # Calculate Metrics per slice\n        for j, mask in enumerate(preds):\n            slice_thick, sp_x, sp_y = batch_metadata[j]\n            \n            # 1. Volume Calculation\n            # Scale factor because we resized the image/mask\n            # Original Area = Mask_Pixels * (Original_W / 256) * (Original_H / 256) * sp_x * sp_y\n            # Simplified approximation: Assume standard scaling, or just use resized pixels as relative volume\n            # To be accurate: Volume = count * (sp_x * 256/Org_W) ... it's complex without org dims.\n            # Robust approx: Volume = Count * sp_x * sp_y * slice_thick (Assuming 256 IS roughly the field of view)\n            # Better: Just sum the raw mask pixels as a \"Relative Volume Score\" for the regression model\n            \n            mask_pixels = np.sum(mask)\n            # Volume contribution (approximate mL)\n            vol = mask_pixels * sp_x * sp_y * slice_thick / 1000.0 \n            patient_metrics['lung_volume_ml'] += vol\n            \n            # 2. Fibrosis Density (HU values inside the mask)\n            # We need the original HU values corresponding to the mask\n            # Since we resized input, we assume the mask corresponds to the resized HU image\n            # Reconstruct Un-normalized HU image for stats\n            img_unnorm = batch_imgs[j] * 1400 - 1000 \n            \n            lung_tissue = img_unnorm[mask == 1]\n            if len(lung_tissue) > 0:\n                hu_values_accumulated.extend(lung_tissue.tolist())\n\n    # Aggregate HU metrics\n    if hu_values_accumulated:\n        # Use simple stats (mean/std) of the tissue\n        # Taking a random subset if too large to save memory\n        if len(hu_values_accumulated) > 10000:\n            arr = np.random.choice(hu_values_accumulated, 10000)\n        else:\n            arr = np.array(hu_values_accumulated)\n            \n        patient_metrics['fibrosis_mean_hu'] = np.mean(arr)\n        patient_metrics['fibrosis_std_hu'] = np.std(arr)\n        \n    return patient_metrics\n\n# ==========================================\n# 4. MAIN EXECUTION\n# ==========================================\ndef extract_features():\n    # Load Model\n    model = StandardUNet().to(CONFIG['device'])\n    try:\n        model.load_state_dict(torch.load(CONFIG['model_weights'], map_location=CONFIG['device']))\n        print(\"‚úÖ U-Net loaded successfully.\")\n    except Exception as e:\n        print(f\"‚ùå Error loading model: {e}\")\n        return\n\n    model.eval()\n    \n    patients = os.listdir(CONFIG['root_dir'])\n    results = []\n    \n    print(\"üöÄ Extracting Biomarkers from all patients...\")\n    for p in tqdm(patients):\n        if os.path.isdir(os.path.join(CONFIG['root_dir'], p)):\n            metrics = process_patient(p, model)\n            results.append(metrics)\n            \n    # Save Results\n    df = pd.DataFrame(results)\n    df.to_csv(\"biomarkers.csv\", index=False)\n    print(\"üéâ Feature Extraction Complete! Saved to 'biomarkers.csv'\")\n    print(df.head())\n\nif __name__ == \"__main__\":\n    extract_features()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:55:14.676849Z","iopub.execute_input":"2025-11-27T10:55:14.678075Z","iopub.status.idle":"2025-11-27T11:09:49.150308Z","shell.execute_reply.started":"2025-11-27T10:55:14.678037Z","shell.execute_reply":"2025-11-27T11:09:49.149460Z"}},"outputs":[{"name":"stdout","text":"‚úÖ U-Net loaded successfully.\nüöÄ Extracting Biomarkers from all patients...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/176 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6bbd7aa6974eeeb2795387f0fe5674"}},"metadata":{}},{"name":"stdout","text":"üéâ Feature Extraction Complete! Saved to 'biomarkers.csv'\n                     Patient  lung_volume_ml  fibrosis_mean_hu  \\\n0  ID00015637202177877247924      532.746878       -654.971900   \n1  ID00035637202182204917484     1020.789423       -586.062900   \n2  ID00290637202279304677843      381.587860       -609.296700   \n3  ID00400637202305055099402     1579.392478       -763.694175   \n4  ID00073637202198167792918     2033.309111       -681.002325   \n\n   fibrosis_std_hu  slice_count  \n0       304.124213          295  \n1       281.396531          574  \n2       284.104577          240  \n3       284.959797          265  \n4       304.808541          355  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}